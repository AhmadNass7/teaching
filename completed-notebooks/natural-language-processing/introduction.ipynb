{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9b1dcd",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Use `NLTK` or `spaCy`, as the main NLP libraries, to process some texts. In particular, you can design a very simple language model as follows: \n",
    "- Choose a piece of text as your training data.\n",
    "- Tokenize sentences and words of the training data.\n",
    "- Count the frequency of each word and store it in a dictionary.\n",
    "- Count the frequency of each bigram and store it in a dictionary. A bigram is the combination of two consecutive words.\n",
    "- Now, you can predict the next word of each sentence using the below probabilistic formula: $$ argmax_w Pr(w|w_{last}) = \\frac{\\text{Frequency of Bigram } (w_{last}, w)}{\\text{Frequency of Word } w_{last}}, $$ where $ w_{last} $ is the last word in the given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e123603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Use NLTK or spaCy, as the main NLP libraries, to process some texts. In particular, you can design a very simple language model as follows:\"\n",
    "\n",
    "d[\"gisma\"] = 4\n",
    "dd[(\"This\", \"is\")] = 3\n",
    "\n",
    "This is Gisma\n",
    "This is\n",
    "is Gisma\n",
    "\n",
    "\n",
    "This is ...\n",
    "\n",
    "is -> 5\n",
    "(is , gimsa) -> 2\n",
    "Pr (gisam|is) = 2 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba290f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tokenizers': 1,\n",
       " 'divide': 1,\n",
       " 'strings': 1,\n",
       " 'into': 1,\n",
       " 'lists': 1,\n",
       " 'of': 1,\n",
       " 'substrings': 1,\n",
       " '.': 1,\n",
       " 'For': 1,\n",
       " 'example': 1,\n",
       " ',': 1,\n",
       " 'tokenizers': 1,\n",
       " 'can': 1,\n",
       " 'be': 1,\n",
       " 'used': 1,\n",
       " 'to': 1,\n",
       " 'find': 1,\n",
       " 'the': 1,\n",
       " 'words': 1,\n",
       " 'and': 1,\n",
       " 'punctuation': 1,\n",
       " 'in': 1,\n",
       " 'a': 1,\n",
       " 'string': 1,\n",
       " ':': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string:\"\n",
    "vocabulary = {}\n",
    "sentences = nltk.tokenize.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    words = nltk.tokenize.word_tokenize(sentence)\n",
    "    for w in words:\n",
    "        if w not in vocabulary:\n",
    "            vocabulary[w] = 0\n",
    "        vocabulary[w] += 1\n",
    "        \n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc47d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13415cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e14714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef16941f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc38b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639eca0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdae1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cba5676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'essentially'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "class SimpleLanguageModeling():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocabulary = {}\n",
    "        self.bigrams = {}\n",
    "\n",
    "    def fit(self, text):\n",
    "        sentences = nltk.tokenize.sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            words = nltk.tokenize.word_tokenize(sentence)\n",
    "            for w in words:\n",
    "                if w not in self.vocabulary:\n",
    "                    self.vocabulary[w] = 0\n",
    "                self.vocabulary[w] += 1\n",
    "            for b in nltk.bigrams(words):\n",
    "                if b not in self.bigrams:\n",
    "                    self.bigrams[b] = 0\n",
    "                self.bigrams[b] += 1\n",
    "        \n",
    "    def predict(self, text):\n",
    "        words = nltk.tokenize.word_tokenize(text)\n",
    "        last_word = words[-1]\n",
    "        if last_word not in self.vocabulary:\n",
    "            return \"\"\n",
    "        max_probability = 0\n",
    "        best_word = \"\"\n",
    "        for w in self.vocabulary:\n",
    "            if (last_word, w) in self.bigrams:\n",
    "                p = self.bigrams[(last_word, w)] / self.vocabulary[last_word]\n",
    "                if p > max_probability:\n",
    "                    max_probability = p\n",
    "                    best_word = w\n",
    "        return best_word\n",
    "\n",
    "\n",
    "    \n",
    "training_text = \"\"\"Training the network is essentially finding a minimum of this multidimensional \"loss\" or \"cost\" function. It's done iteratively over many training runs, incrementally changing the network's state. In practice, that entails making many small adjustments to the network's weights based on the outputs that are computed for a random set of input examples, each time starting with the weights that control the output layer and moving backward through the network. (Only the connections to a single neuron in each layer are shown here, for simplicity.) This backpropagation process is repeated over many random sets of training examples until the loss function is minimized, and the network then provides the best results it can for any new input.\"\"\"\n",
    "model = SimpleLanguageModeling()\n",
    "model.fit(training_text)\n",
    "model.predict(\"the network is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4482d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
